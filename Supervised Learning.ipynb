{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Learning\n",
    "Supervised Learning means when the past experiences that you have (input), are actually labelled with the output. The data that you are given is labelled. The output is labelled for you. Two examples that are most often discussed are predicting house prices and predicting the type of tumor. There are a lot of other applications for supervised learning other than these two. How do you figure out whether something is supervised learning or unsupervised learning? The basic idea is that given any training data, you are given the output corresponding to that. Now, within supervised learning, we have two types of supervised learning:\n",
    "1. Regression\n",
    "    - Regression basically means that the output is a continuous range. For example, the house prices. You are given a lot of features about a house, you are required to predict the price of the house. Let's say we create three categories, where price falls in ranges: price < 100k, 100k < price < 1 million, price > 1 million. Now, this is a classification problem, because you are supposed to put the house into one of the three classes. But if that's not the case, and you're supposed to actually predict the exact amount for the price of the house, then it is a regression problem. So, when you have a continuous spectrum for the output, we are talking about regression. So, the house pricing problem is a regression supervised learning problem. \n",
    "2. Classification\n",
    "    - Classification basically means, where the output is one of the few classes. For example, in the tumor case, we are saying whether the tumor is 'Malignant' or 'Benign'. So we have two classes, and for the input data you're given one of the two classes, and for the output data you're trying to predict the same. Another example of classification, let's say I've given a lot of images, and the images have numbers from 0 to 9. So, what is the input data? Input data is basically images, and we know for each image, whether it is one number with 0 to 9 inclusive. Our task will be, given a new image, predict what number/digit does it contain. That's very clearly supervised learning and it is a classification problem because we are supposed to put the data from 0 to 9 in one of the 10 classes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps for Supervised Learning\n",
    "So now let's talk about what exactly do we need to do when we are given a supervised learning problem. Whenever you'll be given a supervised learning problem, what you'll be given is basically some training data. This training data will consist of say n + 1 columns, where n of these columns are your features (or input characteristics), and the last one is the column for output. Let's say we are given m such rows of data. So we have m training data points, and each datapoint has n features, and one output associated with it. So that's how generally, you'd be given the data.\n",
    "\n",
    "![](images/SupervisedLearning1.png)\n",
    "\n",
    "Now what exactly will be the steps that we will take to make our machine learning model or algorithm.\n",
    "1. Find data \n",
    "    - The first step will be where do I get my data from. Most of the times we will start with a problem we want to solve, let's say we want to build a recommendation enginefor resturants. That would mean I will need a lot of data around what kind of resturants what kind of people like. If I want to make a movie recommendation system then I need to know what kind of movies has somebody recommended, upvoted, downvoted, and all of those things. So I need to find where to get exactly the data I'm looking for to solve the problem at hand. So example, often the data is provided to you to solve the problem at hand. But, that's not going to be the case in real life. In real life, you'll always have to figure out a way to find the data. That data might come from 5 different sources and you might want to collect them from al those sources. It might be coming from just one source, it might be easily available to you because it might be the data of the company you're working for. So, it depends on the type of the problem you're trying to solve, you will have to first solve the problem of where do I get my data from.\n",
    "2. Data Loading and Cleaning \n",
    "    - The second step will be data loading and cleaning. Loading is not that big a problem if data is coming from a single source. But, let's say that the data is coming from 5 different sources. now one source might have some specific feature that might be missing at the other place and so on. The data might not be consistent, so you will have to figure out ways to how to make the whole thing consistent. You might decide that I'll take only the common features. Second step we'll have to perform is of cleaning. Cleaning will consist of taking care of missing data, moving data to one form (strings to integers), adding/removing columns etc.\n",
    "3. Train the Algorithm\n",
    "    - There's actually a step in between which is pick your algorithm. But, mostly we'll just pick any and then train it and then eventually trial and error with few algorithms.\n",
    "4. Test the Algorithm\n",
    "    - We'll always put some mind into which one will work, and which won't, but we'll pick an algorithm, train it and then test it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's talk about how we will be working with data.\n",
    "- Generally when you are given a project, which has 100 data points i.e. m is 100. But you will be provided with no more than 80 data point entries. The idea is that the remaining 20 are kept aside so that the results of algorithm can be evaluated on those 20 data point entries. That's how generally your work gets evaluated in machine learning.\n",
    "- Now, let's say you have got 2 algorithm options Algorithm 1 and Algorithm 2 and you give all of the 80 data point entries to Algorithm 1. So, Algorithm 1 has learned and trained from this data. Now, how will you test whether it has trained well or not. Similarly, you gave all the 80 data point entries to Algorithm 2 to train. How do you figure out whether they work well or not, or which one of the two Algorithms works better?\n",
    "    - One way is, that I have trained on the 80 (given X, Y both). Let's give the same 80, but only give the X and let it predict the Y which is represented by Y^ called Y predicted. This is done for both algorithms. What you can do is that you can compare the Y predicted with the actual output i.e. Y and see how good or bad this algorithm is performing. What we are doing is, whatever training data we had, we gave it to the algorithm, and then we are testing on the same training data to check whether the algorithm is good or bad. That's actually a bad idea. It's bad because it's like saying I've already told you the answer, now predict on the input. You're giving the same input on which you have trained. So, even if the algorithm is as bad as remembering everything, all that you give it to, it will give 100% result. Because if it rememebers everything you give it to, for the X it will find the data in the storage and it's going to give you the exact Y and you will feel that your algorithm is super awesome. In reality, it might be really, really bad, because if I give it some another data say X^, which you have not seen in training, it might perform really bad.\n",
    "\n",
    "![](images/SupervisedLearning2.png)\n",
    "\n",
    "The idea is not to perform well on the past experience the idea is that in future, the algorithm should be able to predict what's going to be the output. That's what the algorithm is supposed to do.\n",
    "\n",
    "- Generally, what should be done is, if you are being provided with 80 data point entries, you should keep some percentage of it aside, let's say 30 percent. So you keep 24 data points aside and you keep 56 as your training data. You don't use the 24 data points which has been kept aside for testing. When you are working on Algorithm 1, you give the 56 data points and when you're working on Algorithm 2, you again give 56 data points. And when you have to test it to find how good a particular algorithm is, you give the 24 data points, which the algorithm has not seen before. So, initially, we were given 80 data points. We had the X as well as Y for the 80 data points. Which we divided into 56 and 24 with their respective X's and Y's. Let's call them X_train, Y_train for 56 data points and X_test, Y_test for the 24 data points. So, you give X_train, Y_train while training the algorithm, but while testing you only give X_test, and you get Y_test_pred (predictions). And now you compare Y_test_pred (predictions) with Y_test. Because Y_test is something that the algorithm has never seen, and if it is performing good on that, it means my algorithm is good. So if an algorithm is going to do really well on training data, but is not going to do well on testing data, that means it's kind of remembering the training data, it's overfitting the training data, it's just trying to match exactly with the training data. And that's going to be a bad solution. We'll always keep some data aside or on hold (often called as holdout data), so that we test the algorithm with that later.\n",
    "\n",
    "![](images/SupervisedLearning3.png)\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
